---
# YAML anchors
s3_credentials: &s3_credentials
  access_key_id: ((s3_access_key_id))
  secret_access_key: ((s3_secret_access_key))
  endpoint: ((s3_endpoint))
  region_name: ((s3_region_name))
  skip_ssl_verification: true
  use_path_style: true

http_resource: &http_resource
  type: http-resource
  source:
    insecure: true

upload_task: &upload_task
  image: platform-automation-vsphere-image
  file: platform-automation-tasks/tasks/upload-product.yml
  params:
    ENV_FILE: body

stage_task: &stage_task
  image: platform-automation-vsphere-image
  file: platform-automation-tasks/tasks/stage-product.yml
  params:
    ENV_FILE: body

configure_task: &configure_task
  image: platform-automation-vsphere-image
  file: platform-automation-tasks/tasks/configure-product.yml
  params:
    CONFIG_FILE: body
    ENV_FILE: body

resource_types:
- name: http-resource
  type: docker-image
  source:
    repository: "((registry_endpoint))/((registry_repo))/jgriff/http-resource"
    tag: 'latest'
    insecure: true
    insecure_registries: [ "((registry_endpoint))" ]

# - name: http-resource
#   type: docker-image
#   source:
#     repository: jgriff/http-resource
#     # this works when provided with a username and password:

resources:
# registry-image Resources
# Note:  local harbor uses self-signed cert
# consider docker-image with insecure_registries
# docker-image is decremented in favor of registry-image
# - name: platform-automation-custom-image
#   type: registry-image
#   source:
#     repository: ((registry_endpoint))/((registry_repo))/platauto-uaac
#     tag: ((platauto_image_tag))
#     insecure: true

# docker-image resources
- name: platform-automation-vsphere-image
  type: docker-image
  source:
    repository: "((registry_endpoint))/((registry_repo))/platform-automation"
    tag: 'latest'
    insecure: true
    insecure_registries: [ "((registry_endpoint))" ]

- name: platform-automation-custom-image
  type: docker-image
  source:
    repository: "((registry_endpoint))/((registry_repo))/platauto-uaac"
    tag: 'latest'
    insecure: true
    insecure_registries: [ "((registry_endpoint))" ]

# s3 Resources
- name: platform-automation-tasks
  type: s3
  source:
    <<: *s3_credentials
    bucket: binaries
    regexp: platform-automation-tasks-(.*).zip

# - name: platform-automation-vsphere-image
#   type: s3
#   source:
#     <<: *s3_credentials
#     bucket: binaries
#     regexp: vsphere-platform-automation-image-(.*).tar.gz

# - name: platform-automation-custom-image
#   type: s3
#   source:
#     <<: *s3_credentials
#     bucket: binaries
#     regexp: platauto-uaac-(.*).tar.gz

- name: stemcell
  type: s3
  source:
    <<: *s3_credentials
    bucket: stemcells
    regexp: bosh-stemcell-(.*)-vsphere.esxi-ubuntu-jammy-go_agent.tgz

# - name: opsman-product
#   type: s3
#   source:
#     <<: *s3_credentials
#     bucket: opsmgr
#     regexp: ops-manager-vsphere-(.*).ova

- name: srt-product
  type: s3
  source:
    <<: *s3_credentials
    bucket: tiles
    regexp: srt-(.*).pivotal

- name: genai-product
  type: s3
  source:
    <<: *s3_credentials
    bucket: tiles
    regexp: genai-(.*).pivotal

- name: postgres-product
  type: s3
  source:
    <<: *s3_credentials
    bucket: tiles
    regexp: postgres-(.*).pivotal

# - name: hub-product
#   type: s3
#   source:
#     <<: *s3_credentials
#     bucket: tiles
#     regexp: tanzu-hub-(\d.*).pivotal

- name: hub-collector-product
  type: s3
  source:
    <<: *s3_credentials
    bucket: tiles
    regexp: hub-tas-collector-(\d.*).pivotal

# HTTP Resources
- name: env
  <<: *http_resource
  source:
    url: ((http_config_env))

# - name: emptystate
#   <<: *http_resource
#   source:
#     url: ((http_config_emptystate))

# - name: config-opsman
#   <<: *http_resource
#   source:
#     url: ((http_config_opsman))

# - name: opsman-ssh-key
#   <<: *http_resource
#   source:
#     url: ((http_opsman_ssh_key))

# - name: config-director
#   <<: *http_resource
#   source:
#     url: ((http_config_director))

# - name: config-director-auth
#   <<: *http_resource
#   source:
#     url: ((http_config_director_auth))

- name: config-genai
  <<: *http_resource
  source:
    url: ((http_config_genai))

- name: config-postgres
  <<: *http_resource
  source:
    url: ((http_config_postgres))

- name: config-srt
  <<: *http_resource
  source:
    url: ((http_config_srt))

# - name: config-hub
#   <<: *http_resource
#   source:
#     url: ((http_config_hub))

- name: config-hub-collector
  <<: *http_resource
  source:
    url: ((http_config_hub_collector))

# - name: config-srt-otel-template
#   <<: *http_resource
#   source:
#     url: ((http_config_srt_otel_template))

# - name: fixhub-script
#   <<: *http_resource
#   source:
#     url: ((http_config_fixhub_script))

groups:
- name: cf-small-footprint
  jobs:
  - cf-small-footprint
  - free-space-on-concourse-cf
  - apply-changes-cf
- name: postgres
  jobs:
  - postgres
  - free-space-on-concourse-postgres
  - apply-changes-postgres
- name: genai
  jobs:
  - genai
  - free-space-on-concourse-genai
  - apply-changes-genai
- name: hub-collector
  jobs:
  - hub-collector
  - free-space-on-concourse-hub-collector
  - apply-changes-hub-collector

jobs:
- name: cf-small-footprint
  build_log_retention:
    builds: 2
  plan:
  - get: env
  - get: config-srt
  - get: srt-product
    trigger: true
  - get: stemcell
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: upload-product
    <<: *upload_task
    input_mapping:
      product: srt-product
      config: config-srt
      env: env

  - task: stage-product
    <<: *stage_task
    input_mapping:
      product: srt-product
      env: env

  - task: upload-stemcell
    image: platform-automation-vsphere-image
    file: platform-automation-tasks/tasks/upload-stemcell.yml
    input_mapping:
      env: env
      stemcell: stemcell
    params:
      ENV_FILE: body

  - task: create-certificate-vars
    image: platform-automation-vsphere-image
    config:
      platform: linux
      inputs:
      - name: env
      params:
        DOMAINS: ((certificate_domains))
        ENV_FILE: body
      outputs:
      - name: vars
      run:
        path: sh
        args:
        - -c
        - |
          echo "generating cert for ${DOMAINS}"
          om --env env/body generate-certificate -d "${DOMAINS}" > /tmp/certificate.json
          om interpolate -c /tmp/certificate.json --path /certificate > /tmp/wildcard-cert.pem
          om interpolate -c /tmp/certificate.json --path /key > /tmp/wildcard-key.pem
          echo "wildcard-cert-pem: |" > vars/certificate-vars.yml
          sed 's/^/  /' /tmp/wildcard-cert.pem >> vars/certificate-vars.yml
          echo "wildcard-key-pem: |" >> vars/certificate-vars.yml
          sed 's/^/  /' /tmp/wildcard-key.pem >> vars/certificate-vars.yml

  - task: configure-product
    image: platform-automation-vsphere-image
    file: platform-automation-tasks/tasks/configure-product.yml
    params:
      CONFIG_FILE: body
      ENV_FILE: body
      VARS_FILES: vars/certificate-vars.yml
    input_mapping:
      config: config-srt
      env: env
      vars: vars

  - task: cleanup
    image: platform-automation-vsphere-image
    config:
      platform: linux
      inputs:
      - name: srt-product
      - name: stemcell
      run:
        path: sh
        args:
        - -exc
        - |
          rm -rf srt-product/*
          rm -rf stemcell/*

- name: free-space-on-concourse-cf
  plan:
  - get: env
    passed: [ cf-small-footprint ]
    trigger: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - task: clear-resource-cache
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        CONCOURSE: ((concourse_url))
        CONCOURSE_USER: "admin"
        CONCOURSE_PASS: ((s3_secret_access_key))
        CONCOURSE_TARGET: "ci"
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          #!/bin/bash
          # Clears cache for resources ending with 'product' that have 'succeeded' status

          echo "Starting Concourse resource cache cleanup..."
          # Check if fly CLI exists, if not download it
          if ! command -v fly &> /dev/null; then
              echo "fly CLI not found, downloading from Concourse..."
              curl "$CONCOURSE/api/v1/cli?arch=amd64&platform=linux" -o fly
              if [ $? -ne 0 ]; then
                  echo "Error: Failed to download fly CLI"
                  exit 1
              fi
              chmod +x fly
              echo "fly CLI downloaded successfully"
              FLY_CMD="./fly"
          else
              echo "fly CLI found in PATH"
              FLY_CMD="fly"
          fi
          # Step 1: Login to Concourse
          echo "Logging in to Concourse at $CONCOURSE..."
          $FLY_CMD -t "$CONCOURSE_TARGET" login -c "$CONCOURSE" -u "$CONCOURSE_USER" -p "$CONCOURSE_PASS"
          if [ $? -ne 0 ]; then
              echo "Error: Failed to login to Concourse"
              exit 1
          fi
          echo "Login successful!"
          # Step 2: Get all pipelines
          echo "Enumerating pipelines..."
          pipelines=$($FLY_CMD -t "$CONCOURSE_TARGET" pipelines --json | jq -r '.[].name')
          if [ -z "$pipelines" ]; then
              echo "No pipelines found"
              exit 0
          fi
          echo "Found pipelines: $pipelines"
          # Step 3 & 4: For each pipeline, get resources and clear cache for matching ones
          for pipeline in $pipelines; do
              echo ""
              echo "Processing pipeline: $pipeline"
              # Get resources for this pipeline
              resources=$($FLY_CMD -t "$CONCOURSE_TARGET" resources -p "$pipeline" --json)
              if [ $? -ne 0 ]; then
                  echo "Warning: Could not get resources for pipeline $pipeline, skipping..."
                  continue
              fi
              # Filter resources that end with 'product' and have 'succeeded' status
              matching_resources=$(echo "$resources" | jq -r '.[] | select(.name | endswith("product")) | select(.build.status == "succeeded") | .name')
              if [ -z "$matching_resources" ]; then
                  echo "  No matching resources found in pipeline $pipeline"
                  continue
              fi
              echo "  Found matching resources in $pipeline:"
              for resource in $matching_resources; do
                  echo "    - $resource"
              done
              # Clear cache for each matching resource
              for resource in $matching_resources; do
                  echo "  Clearing cache for $pipeline/$resource..."
                  echo "y" | $FLY_CMD -t "$CONCOURSE_TARGET" clear-resource-cache -r "$pipeline/$resource"
                  if [ $? -eq 0 ]; then
                      echo "    ✓ Successfully cleared cache for $pipeline/$resource"
                  else
                      echo "    ✗ Failed to clear cache for $pipeline/$resource"
                  fi
              done
          done
          echo "Cache cleanup completed!"

  - task: delete-pivotal-files-from-worker
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        DOCKER_HOST: ((docker_host))
        WORKER_NAME: concourse-worker-1
        DOCKER_PASS: ((s3_secret_access_key))
        CONCOURSE: ((concourse_url))
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          echo "Connecting to $DOCKER_HOST to delete pivotal files from worker $WORKER_NAME"
          # Quote the pattern to avoid local glob expansion. Redirect remote stderr so permission
          # denied messages don't cause the local script to fail. Use '|| true' so ssh returns 0
          # even if find had non-fatal problems deleting some files.
          remote_command="find /opt/concourse/worker/volumes -name '*pivotal*' -type f -delete 2>/dev/null || true"
          sshpass -p "$DOCKER_PASS" ssh -o StrictHostKeyChecking=no "ubuntu@$DOCKER_HOST" "$remote_command"

- name: apply-changes-cf
  serial: true
  serial_groups: [ apply-changes ]
  build_log_retention:
    builds: 1
  plan:
  - get: env
    passed: [ free-space-on-concourse-cf ]
    trigger: true
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: apply-product-changes
    image: platform-automation-vsphere-image
    file: platform-automation-tasks/tasks/apply-changes.yml
    input_mapping:
      env: env
    params:
      ENV_FILE: body
      SELECTIVE_DEPLOY_PRODUCTS: "cf"

- name: postgres
  serial: false
  build_log_retention:
    builds: 1
  plan:
  - get: postgres-product
    trigger: true
  - get: config-postgres
  - get: env
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: upload-product
    <<: *upload_task
    input_mapping:
      product: postgres-product
      config: config-postgres
      env: env

  - task: stage-product
    <<: *stage_task
    input_mapping:
      product: postgres-product
      env: env

  - task: configure-product
    <<: *configure_task
    input_mapping:
      config: config-postgres
      env: env
      #vars: interpolated-creds

  - task: cleanup
    image: platform-automation-vsphere-image
    config:
      platform: linux
      inputs:
      - name: postgres-product
      run:
        path: sh
        args:
        - -exc
        - |
          rm -rf postgres-product/*

- name: free-space-on-concourse-postgres
  plan:
  - get: env
    passed: [ postgres ]
    trigger: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - task: clear-resource-cache
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        CONCOURSE: ((concourse_url))
        CONCOURSE_USER: "admin"
        CONCOURSE_PASS: ((s3_secret_access_key))
        CONCOURSE_TARGET: "ci"
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          #!/bin/bash
          # Clears cache for resources ending with 'product' that have 'succeeded' status

          echo "Starting Concourse resource cache cleanup..."
          # Check if fly CLI exists, if not download it
          if ! command -v fly &> /dev/null; then
              echo "fly CLI not found, downloading from Concourse..."
              curl "$CONCOURSE/api/v1/cli?arch=amd64&platform=linux" -o fly
              if [ $? -ne 0 ]; then
                  echo "Error: Failed to download fly CLI"
                  exit 1
              fi
              chmod +x fly
              echo "fly CLI downloaded successfully"
              FLY_CMD="./fly"
          else
              echo "fly CLI found in PATH"
              FLY_CMD="fly"
          fi
          # Step 1: Login to Concourse
          echo "Logging in to Concourse at $CONCOURSE..."
          $FLY_CMD -t "$CONCOURSE_TARGET" login -c "$CONCOURSE" -u "$CONCOURSE_USER" -p "$CONCOURSE_PASS"
          if [ $? -ne 0 ]; then
              echo "Error: Failed to login to Concourse"
              exit 1
          fi
          echo "Login successful!"
          # Step 2: Get all pipelines
          echo "Enumerating pipelines..."
          pipelines=$($FLY_CMD -t "$CONCOURSE_TARGET" pipelines --json | jq -r '.[].name')
          if [ -z "$pipelines" ]; then
              echo "No pipelines found"
              exit 0
          fi
          echo "Found pipelines: $pipelines"
          # Step 3 & 4: For each pipeline, get resources and clear cache for matching ones
          for pipeline in $pipelines; do
              echo ""
              echo "Processing pipeline: $pipeline"
              # Get resources for this pipeline
              resources=$($FLY_CMD -t "$CONCOURSE_TARGET" resources -p "$pipeline" --json)
              if [ $? -ne 0 ]; then
                  echo "Warning: Could not get resources for pipeline $pipeline, skipping..."
                  continue
              fi
              # Filter resources that end with 'product' and have 'succeeded' status
              matching_resources=$(echo "$resources" | jq -r '.[] | select(.name | endswith("product")) | select(.build.status == "succeeded") | .name')
              if [ -z "$matching_resources" ]; then
                  echo "  No matching resources found in pipeline $pipeline"
                  continue
              fi
              echo "  Found matching resources in $pipeline:"
              for resource in $matching_resources; do
                  echo "    - $resource"
              done
              # Clear cache for each matching resource
              for resource in $matching_resources; do
                  echo "  Clearing cache for $pipeline/$resource..."
                  echo "y" | $FLY_CMD -t "$CONCOURSE_TARGET" clear-resource-cache -r "$pipeline/$resource"
                  if [ $? -eq 0 ]; then
                      echo "    ✓ Successfully cleared cache for $pipeline/$resource"
                  else
                      echo "    ✗ Failed to clear cache for $pipeline/$resource"
                  fi
              done
          done
          echo "Cache cleanup completed!"

  - task: delete-pivotal-files-from-worker
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        DOCKER_HOST: ((docker_host))
        WORKER_NAME: concourse-worker-1
        DOCKER_PASS: ((s3_secret_access_key))
        CONCOURSE: ((concourse_url))
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          echo "Connecting to $DOCKER_HOST to delete pivotal files from worker $WORKER_NAME"
          # Quote the pattern to avoid local glob expansion. Redirect remote stderr so permission
          # denied messages don't cause the local script to fail. Use '|| true' so ssh returns 0
          # even if find had non-fatal problems deleting some files.
          remote_command="find /opt/concourse/worker/volumes -name '*pivotal*' -type f -delete 2>/dev/null || true"
          sshpass -p "$DOCKER_PASS" ssh -o StrictHostKeyChecking=no "ubuntu@$DOCKER_HOST" "$remote_command"

- name: apply-changes-postgres
  serial: true
  serial_groups: [ apply-changes ]
  build_log_retention:
    builds: 1
  plan:
  - get: env
    passed: [ free-space-on-concourse-postgres ]
    trigger: true
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: apply-product-changes
    image: platform-automation-vsphere-image
    file: platform-automation-tasks/tasks/apply-changes.yml
    input_mapping:
      env: env
    params:
      ENV_FILE: body
      SELECTIVE_DEPLOY_PRODUCTS: "cf,postgres"

- name: genai
  serial: false
  #serial_groups: [ phase_two ]
  build_log_retention:
    builds: 1
  plan:
  - get: genai-product
    trigger: true
  - get: config-genai
  - get: env
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: upload-product
    <<: *upload_task
    input_mapping:
      product: genai-product
      config: config-genai
      env: env

  - task: stage-product
    <<: *stage_task
    input_mapping:
      product: genai-product
      env: env

  - task: configure-product
    <<: *configure_task
    input_mapping:
      config: config-genai
      env: env

  - task: cleanup
    image: platform-automation-vsphere-image
    config:
      platform: linux
      inputs:
      - name: genai-product
      run:
        path: sh
        args:
        - -exc
        - |
          rm -rf genai-product/*

- name: free-space-on-concourse-genai
  plan:
  - get: env
    passed: [ genai ]
    trigger: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - task: clear-resource-cache
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        CONCOURSE: ((concourse_url))
        CONCOURSE_USER: "admin"
        CONCOURSE_PASS: ((s3_secret_access_key))
        CONCOURSE_TARGET: "ci"
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          #!/bin/bash
          # Clears cache for resources ending with 'product' that have 'succeeded' status

          echo "Starting Concourse resource cache cleanup..."
          # Check if fly CLI exists, if not download it
          if ! command -v fly &> /dev/null; then
              echo "fly CLI not found, downloading from Concourse..."
              curl "$CONCOURSE/api/v1/cli?arch=amd64&platform=linux" -o fly
              if [ $? -ne 0 ]; then
                  echo "Error: Failed to download fly CLI"
                  exit 1
              fi
              chmod +x fly
              echo "fly CLI downloaded successfully"
              FLY_CMD="./fly"
          else
              echo "fly CLI found in PATH"
              FLY_CMD="fly"
          fi
          # Step 1: Login to Concourse
          echo "Logging in to Concourse at $CONCOURSE..."
          $FLY_CMD -t "$CONCOURSE_TARGET" login -c "$CONCOURSE" -u "$CONCOURSE_USER" -p "$CONCOURSE_PASS"
          if [ $? -ne 0 ]; then
              echo "Error: Failed to login to Concourse"
              exit 1
          fi
          echo "Login successful!"
          # Step 2: Get all pipelines
          echo "Enumerating pipelines..."
          pipelines=$($FLY_CMD -t "$CONCOURSE_TARGET" pipelines --json | jq -r '.[].name')
          if [ -z "$pipelines" ]; then
              echo "No pipelines found"
              exit 0
          fi
          echo "Found pipelines: $pipelines"
          # Step 3 & 4: For each pipeline, get resources and clear cache for matching ones
          for pipeline in $pipelines; do
              echo ""
              echo "Processing pipeline: $pipeline"
              # Get resources for this pipeline
              resources=$($FLY_CMD -t "$CONCOURSE_TARGET" resources -p "$pipeline" --json)
              if [ $? -ne 0 ]; then
                  echo "Warning: Could not get resources for pipeline $pipeline, skipping..."
                  continue
              fi
              # Filter resources that end with 'product' and have 'succeeded' status
              matching_resources=$(echo "$resources" | jq -r '.[] | select(.name | endswith("product")) | select(.build.status == "succeeded") | .name')
              if [ -z "$matching_resources" ]; then
                  echo "  No matching resources found in pipeline $pipeline"
                  continue
              fi
              echo "  Found matching resources in $pipeline:"
              for resource in $matching_resources; do
                  echo "    - $resource"
              done
              # Clear cache for each matching resource
              for resource in $matching_resources; do
                  echo "  Clearing cache for $pipeline/$resource..."
                  echo "y" | $FLY_CMD -t "$CONCOURSE_TARGET" clear-resource-cache -r "$pipeline/$resource"
                  if [ $? -eq 0 ]; then
                      echo "    ✓ Successfully cleared cache for $pipeline/$resource"
                  else
                      echo "    ✗ Failed to clear cache for $pipeline/$resource"
                  fi
              done
          done
          echo "Cache cleanup completed!"

  - task: delete-pivotal-files-from-worker
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        DOCKER_HOST: ((docker_host))
        WORKER_NAME: concourse-worker-1
        DOCKER_PASS: ((s3_secret_access_key))
        CONCOURSE: ((concourse_url))
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          echo "Connecting to $DOCKER_HOST to delete pivotal files from worker $WORKER_NAME"
          # Quote the pattern to avoid local glob expansion. Redirect remote stderr so permission
          # denied messages don't cause the local script to fail. Use '|| true' so ssh returns 0
          # even if find had non-fatal problems deleting some files.
          remote_command="find /opt/concourse/worker/volumes -name '*pivotal*' -type f -delete 2>/dev/null || true"
          sshpass -p "$DOCKER_PASS" ssh -o StrictHostKeyChecking=no "ubuntu@$DOCKER_HOST" "$remote_command"

- name: apply-changes-genai
  serial: true
  serial_groups: [ apply-changes ]
  build_log_retention:
    builds: 1
  plan:
  - get: env
    passed: [ free-space-on-concourse-genai ]
    trigger: true
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: apply-product-changes
    image: platform-automation-vsphere-image
    file: platform-automation-tasks/tasks/apply-changes.yml
    input_mapping:
      env: env
    params:
      ENV_FILE: body
      SELECTIVE_DEPLOY_PRODUCTS: "cf,genai"

- name: hub-collector
  serial_groups: [ phase_two ]
  build_log_retention:
    builds: 1
  plan:
  - get: env
  - get: config-hub-collector
  - get: hub-collector-product
    trigger: true
  - get: stemcell
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: upload-product
    <<: *upload_task
    input_mapping:
      product: hub-collector-product
      config: config-hub-collector
      env: env

  - task: cleanup
    image: platform-automation-vsphere-image
    config:
      platform: linux
      inputs:
      - name: hub-collector-product
      run:
        path: sh
        args:
        - -exc
        - |
          rm -rf hub-collector-product/*

- name: free-space-on-concourse-hub-collector
  plan:
  - get: env
    passed: [ hub-collector ]
    trigger: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - task: clear-resource-cache
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        CONCOURSE: ((concourse_url))
        CONCOURSE_USER: "admin"
        CONCOURSE_PASS: ((s3_secret_access_key))
        CONCOURSE_TARGET: "ci"
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          #!/bin/bash
          # Clears cache for resources ending with 'product' that have 'succeeded' status

          echo "Starting Concourse resource cache cleanup..."
          # Check if fly CLI exists, if not download it
          if ! command -v fly &> /dev/null; then
              echo "fly CLI not found, downloading from Concourse..."
              curl "$CONCOURSE/api/v1/cli?arch=amd64&platform=linux" -o fly
              if [ $? -ne 0 ]; then
                  echo "Error: Failed to download fly CLI"
                  exit 1
              fi
              chmod +x fly
              echo "fly CLI downloaded successfully"
              FLY_CMD="./fly"
          else
              echo "fly CLI found in PATH"
              FLY_CMD="fly"
          fi
          # Step 1: Login to Concourse
          echo "Logging in to Concourse at $CONCOURSE..."
          $FLY_CMD -t "$CONCOURSE_TARGET" login -c "$CONCOURSE" -u "$CONCOURSE_USER" -p "$CONCOURSE_PASS"
          if [ $? -ne 0 ]; then
              echo "Error: Failed to login to Concourse"
              exit 1
          fi
          echo "Login successful!"
          # Step 2: Get all pipelines
          echo "Enumerating pipelines..."
          pipelines=$($FLY_CMD -t "$CONCOURSE_TARGET" pipelines --json | jq -r '.[].name')
          if [ -z "$pipelines" ]; then
              echo "No pipelines found"
              exit 0
          fi
          echo "Found pipelines: $pipelines"
          # Step 3 & 4: For each pipeline, get resources and clear cache for matching ones
          for pipeline in $pipelines; do
              echo ""
              echo "Processing pipeline: $pipeline"
              # Get resources for this pipeline
              resources=$($FLY_CMD -t "$CONCOURSE_TARGET" resources -p "$pipeline" --json)
              if [ $? -ne 0 ]; then
                  echo "Warning: Could not get resources for pipeline $pipeline, skipping..."
                  continue
              fi
              # Filter resources that end with 'product' and have 'succeeded' status
              matching_resources=$(echo "$resources" | jq -r '.[] | select(.name | endswith("product")) | select(.build.status == "succeeded") | .name')
              if [ -z "$matching_resources" ]; then
                  echo "  No matching resources found in pipeline $pipeline"
                  continue
              fi
              echo "  Found matching resources in $pipeline:"
              for resource in $matching_resources; do
                  echo "    - $resource"
              done
              # Clear cache for each matching resource
              for resource in $matching_resources; do
                  echo "  Clearing cache for $pipeline/$resource..."
                  echo "y" | $FLY_CMD -t "$CONCOURSE_TARGET" clear-resource-cache -r "$pipeline/$resource"
                  if [ $? -eq 0 ]; then
                      echo "    ✓ Successfully cleared cache for $pipeline/$resource"
                  else
                      echo "    ✗ Failed to clear cache for $pipeline/$resource"
                  fi
              done
          done
          echo "Cache cleanup completed!"

  - task: delete-pivotal-files-from-worker
    image: platform-automation-custom-image
    config:
      platform: linux
      params:
        DOCKER_HOST: ((docker_host))
        WORKER_NAME: concourse-worker-1
        DOCKER_PASS: ((s3_secret_access_key))
        CONCOURSE: ((concourse_url))
      run:
        path: bash
        args:
        - -c
        - |
          set -eu
          echo "Connecting to $DOCKER_HOST to delete pivotal files from worker $WORKER_NAME"
          # Quote the pattern to avoid local glob expansion. Redirect remote stderr so permission
          # denied messages don't cause the local script to fail. Use '|| true' so ssh returns 0
          # even if find had non-fatal problems deleting some files.
          remote_command="find /opt/concourse/worker/volumes -name '*pivotal*' -type f -delete 2>/dev/null || true"
          sshpass -p "$DOCKER_PASS" ssh -o StrictHostKeyChecking=no "ubuntu@$DOCKER_HOST" "$remote_command"

- name: apply-changes-hub-collector
  serial: true
  serial_groups: [ apply-changes ]
  build_log_retention:
    builds: 1
  plan:
  - get: env
    passed: [ free-space-on-concourse-hub-collector ]
    trigger: true
  - get: platform-automation-vsphere-image
    params:
      unpack: true
  - get: platform-automation-custom-image
    #params:
    #  unpack: true
  - get: platform-automation-tasks
    params:
      unpack: true

  - task: apply-product-changes
    image: platform-automation-vsphere-image
    file: platform-automation-tasks/tasks/apply-changes.yml
    input_mapping:
      env: env
    params:
      ENV_FILE: body
      SELECTIVE_DEPLOY_PRODUCTS: "cf,hub-tas-collector"
